{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Moving Average Rewards: 0.0, Episode: 23."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-81a690c16cf4>:937: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  reward = torch.tensor([reward], dtype=torch.int64).to(device)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-81a690c16cf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    944\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m                 \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m                 \u001b[0mcurrent_q_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQValues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_current\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m                 \u001b[0mnext_q_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQValues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m                 \u001b[0mtarget_q_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnext_q_values\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-81a690c16cf4>\u001b[0m in \u001b[0;36mget_current\u001b[1;34m(policy_net, states, actions)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_current\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-81a690c16cf4>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Advantage Stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1136\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1137\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "import time\n",
    "from ipywidgets.widgets.interaction import show_inline_matplotlib_plots\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import pickle\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "out = display(IPython.display.Pretty('Starting'), display_id=True)\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, NUMBER_OF_ARGUMENTS_PER_STATE):\n",
    "        super().__init__(),\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=NUMBER_OF_ARGUMENTS_PER_STATE, out_features=128) \n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=128) \n",
    "        self.out_v = nn.Linear(in_features=128, out_features=1)\n",
    "        self.out_a = nn.Linear(in_features=128, out_features=32)\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = t.flatten(start_dim=1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        v = self.out_v(t) #Value Stream\n",
    "        a = self.out_a(t) # Advantage Stream\n",
    "        q = v + a - a.mean()\n",
    "        return q\n",
    "\n",
    "\n",
    "Experience = namedtuple(\n",
    "            'Experience',\n",
    "            ('state', 'action', 'next_state', 'reward')\n",
    "                        )\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size\n",
    "\n",
    "class EpsilonGreedyStrategy():\n",
    "\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "                            math.exp(-1. * current_step / self.decay)\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "\n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            return torch.tensor([action]).to(self.device) # explore    \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).argmax(dim=1).to(self.device) # exploit\n",
    "\n",
    "class QValues():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states):                 \n",
    "        return target_net(next_states).max(dim=1)[0].detach()\n",
    "\n",
    "def plot(values,r1,r2,r3,r4,r5,r6,h1,h2,h3,h4,a1,a2,a3,a4,SUM1,SUM2,SUM3,SUM4,Fairness,H,AVG2, Fairness2, moving_avg_period):\n",
    "\n",
    "\n",
    "    moving_avg_rewards = get_moving_average(moving_avg_period, values)\n",
    "\n",
    "    if episode == 999:\n",
    "\n",
    "        Fairness = [element * 100 for element in Fairness]\n",
    "        moving_avg_fairness = get_moving_average(moving_avg_period, Fairness)\n",
    "        Fairness2 = [element * 100 for element in Fairness2]\n",
    "        moving_avg_fairness2 = get_moving_average(moving_avg_period, Fairness2)\n",
    "\n",
    "        moving_avg_h1 = get_moving_average(moving_avg_period, h1)\n",
    "        moving_avg_h2 = get_moving_average(moving_avg_period, h2)\n",
    "\n",
    "        moving_avg_h3 = get_moving_average(moving_avg_period, h3)\n",
    "        moving_avg_h4 = get_moving_average(moving_avg_period, h4)\n",
    "\n",
    "        moving_avg_a1 = get_moving_average(moving_avg_period, a1)\n",
    "        moving_avg_a2 = get_moving_average(moving_avg_period, a2)\n",
    "        moving_avg_a1 = [element * 100 for element in moving_avg_a1]\n",
    "        moving_avg_a2 = [element * 100 for element in moving_avg_a2]\n",
    "\n",
    "        moving_avg_a3 = get_moving_average(moving_avg_period, a3)\n",
    "        moving_avg_a4 = get_moving_average(moving_avg_period, a4)\n",
    "        moving_avg_a3 = [element * 100 for element in moving_avg_a3]\n",
    "        moving_avg_a4 = [element * 100 for element in moving_avg_a4]\n",
    "\n",
    "        \n",
    "        moving_avg_SUM1 = get_moving_average(moving_avg_period, SUM1)\n",
    "        moving_avg_SUM2 = get_moving_average(moving_avg_period, SUM2)\n",
    "        moving_avg_SUM1 = [element * 2000 for element in moving_avg_SUM1]\n",
    "        moving_avg_SUM2 = [element * 2000 for element in moving_avg_SUM2]\n",
    "\n",
    "        moving_avg_SUM3 = get_moving_average(moving_avg_period, SUM3)\n",
    "        moving_avg_SUM4 = get_moving_average(moving_avg_period, SUM4)\n",
    "        moving_avg_SUM3 = [element * 2000 for element in moving_avg_SUM3]\n",
    "        moving_avg_SUM4 = [element * 2000 for element in moving_avg_SUM4]\n",
    "\n",
    "        SUM = np.add(moving_avg_SUM1,moving_avg_SUM2)\n",
    "        SUM = np.add(SUM,moving_avg_SUM3)\n",
    "        SUM = np.add(SUM,moving_avg_SUM4)\n",
    "        avg2 = get_moving_average(moving_avg_period, AVG2)\n",
    "        avg2 = [element * 2000 for element in avg2]\n",
    "      \n",
    "        moving_avg_Height = get_moving_average(moving_avg_period, H)\n",
    "        \n",
    "\n",
    "        P = 10**((p-30)/10)\n",
    "\n",
    "        print(\"\\np = \", p, \"dBm\")\n",
    "        print(moving_avg_period, \"Episode moving avg:\", moving_avg_rewards[-1], \"Sum Rate Moving Average:\",round(SUM[-1],2)/1000, \"Total SE = \", round(SUM[-1]/(2000),2), \"Gbps, EE = \", round(SUM[-1]/(2000 * P),2))\n",
    "        print(\"SE1 = \", round(moving_avg_SUM1[-1]/2000, 2) , \"SE2 = \", round(moving_avg_SUM2[-1]/2000, 2), \"SE3 = \",round(moving_avg_SUM3[-1]/2000, 2), \"SE4 = \", round(moving_avg_SUM4[-1]/2000, 2), \"\\n\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        out.update(IPython.display.Pretty(f'Moving Average Rewards: {round(moving_avg_rewards[-1], 2)}, Episode: {len(SUM1)}.'))\n",
    "\n",
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()\n",
    "    \n",
    "def mmLineOfSight_Check(D,H):\n",
    "    L = 1\n",
    "    return L\n",
    "    C = 9.6117 # Urban LOS probability parameter \n",
    "    Y = 0.1581 # Urban LOS probability parameter\n",
    "    RAND = random.uniform(0,1)\n",
    "    teta = math.asin(H/D) * 180/math.pi\n",
    "    p1 = 1 / ( 1 + (C * math.exp( -Y * (teta - C ) ) ) )\n",
    "    p2 = 1 - p1\n",
    "    if p1 >= p2:\n",
    "        if RAND >= p2:\n",
    "            L = 1\n",
    "        else:\n",
    "            L = 2\n",
    "    else:\n",
    "        if RAND >= p1:\n",
    "            L = 2\n",
    "        else:\n",
    "            L = 1\n",
    "    return L\n",
    "    \n",
    "def Average(lst): \n",
    "    return sum(lst) / len(lst) \n",
    "\n",
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)\n",
    "\n",
    "class Blob():\n",
    "    def __init__(self, size, USER1=False, USER2=False, USER3=False, USER4=False):\n",
    "        self.size = size\n",
    "        if USER1:\n",
    "            self.x = 35\n",
    "            self.y = 54\n",
    "        elif USER2:\n",
    "            self.x = 94\n",
    "            self.y = 1\n",
    "        elif USER3:\n",
    "            self.x = 29\n",
    "            self.y = 45\n",
    "        elif USER4:\n",
    "            self.x = 1\n",
    "            self.y = 97\n",
    "        else:\n",
    "            self.x = 50\n",
    "            self.y = 50\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Blob({self.x}, {self.y})\"\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return [(self.x-other.x)/10, (self.y-other.y)/10]\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "\n",
    "    def action(self, choice):\n",
    "        \n",
    "\n",
    "        if choice == 0:\n",
    "            self.move(x=1, y=1)\n",
    "            self.a1 += 0.01\n",
    "            self.a3 +=0.01\n",
    "            self.H += 1\n",
    "\n",
    "        elif choice == 1:\n",
    "            self.move(x=-1, y=-1)\n",
    "            self.a1 += 0.01\n",
    "            self.a3 +=0.01\n",
    "            self.H += 1\n",
    "\n",
    "        elif choice == 2:\n",
    "            self.move(x=-1, y=1)\n",
    "            self.a1 += 0.01\n",
    "            self.a3 +=0.01\n",
    "            self.H += 1\n",
    "\n",
    "        elif choice == 3:\n",
    "            self.move(x=1, y=-1)\n",
    "            self.a1 += 0.01\n",
    "            self.a3 +=0.01\n",
    "            self.H += 1\n",
    "            \n",
    "        elif choice == 4:\n",
    "            self.move(x=1, y=1)\n",
    "            self.a1 += 0.01\n",
    "            self.a3 -=0.01\n",
    "            self.H += 1\n",
    "\n",
    "        elif choice == 5:\n",
    "            self.move(x=-1, y=-1)\n",
    "            self.a1 += 0.01\n",
    "            self.a3 -= 0.01\n",
    "            self.H += 1\n",
    "\n",
    "        elif choice == 6:\n",
    "            self.move(x=-1, y=1)\n",
    "            self.a1 += 0.01\n",
    "            self.a3 -= 0.01\n",
    "            self.H += 1\n",
    "\n",
    "        elif choice == 7:\n",
    "            self.move(x=1, y=-1)\n",
    "            self.a1 += 0.01\n",
    "            self.a3 -= 0.01\n",
    "            self.H += 1\n",
    "            \n",
    "        elif choice == 8:\n",
    "            self.move(x=1, y=1)\n",
    "            self.a1 -= 0.01\n",
    "            self.a3 += 0.01\n",
    "            self.H += 1\n",
    "            \n",
    "        elif choice == 9:\n",
    "            self.move(x=-1, y=-1)\n",
    "            self.a1 -= 0.01\n",
    "            self.a3 += 0.01\n",
    "            self.H += 1\n",
    "\n",
    "        elif choice == 10:\n",
    "            self.a1 -= 0.01\n",
    "            self.a3 += 0.01\n",
    "            self.H += 1\n",
    "\n",
    "        elif choice == 11:\n",
    "            self.move(x=1, y=-1)\n",
    "            self.a1 -= 0.01\n",
    "            self.a3 += 0.01\n",
    "            self.H += 1\n",
    "            \n",
    "        elif choice == 12:\n",
    "            self.move(x=1, y=1)\n",
    "            self.a1 -= 0.01\n",
    "            self.a3 -= 0.01\n",
    "            self.H += 1\n",
    "\n",
    "        elif choice == 13:\n",
    "            self.move(x=-1, y=-1)\n",
    "            self.a1 -= 0.01\n",
    "            self.a3 -= 0.01\n",
    "            self.H += 1\n",
    "\n",
    "        elif choice == 14:\n",
    "            self.move(x=-1, y=1)\n",
    "            self.a1 -= 0.01\n",
    "            self.a3 -= 0.01\n",
    "            self.H += 1\n",
    "\n",
    "        elif choice == 15:\n",
    "            self.move(x=1, y=-1)\n",
    "            self.a1 -= 0.01\n",
    "            self.a3 -= 0.01\n",
    "            self.H += 1\n",
    "            \n",
    "        if choice == 16:\n",
    "            self.move(x=1, y=1)\n",
    "            self.a1 += 0.01\n",
    "            self.a3 +=0.01\n",
    "            self.H -= 1\n",
    "\n",
    "        elif choice == 17:\n",
    "            self.move(x=-1, y=-1)\n",
    "            self.a1 += 0.01\n",
    "            self.a3 +=0.01\n",
    "            self.H -= 1\n",
    "\n",
    "        elif choice == 18:\n",
    "            self.move(x=-1, y=1)\n",
    "            self.a1 += 0.01\n",
    "            self.a3 +=0.01\n",
    "            self.H -= 1\n",
    "\n",
    "        elif choice == 19:\n",
    "            self.move(x=1, y=-1)\n",
    "            self.a1 += 0.01\n",
    "            self.a3 +=0.01\n",
    "            self.H -= 1\n",
    "            \n",
    "        elif choice == 20:\n",
    "            self.move(x=1, y=1)\n",
    "            self.a1 += 0.01\n",
    "            self.a3 -=0.01\n",
    "            self.H -= 1\n",
    "\n",
    "        elif choice == 21:\n",
    "            self.move(x=-1, y=-1)\n",
    "            self.a1 += 0.01\n",
    "            self.a3 -= 0.01\n",
    "            self.H -= 1\n",
    "\n",
    "        elif choice == 22:\n",
    "            self.move(x=-1, y=1)\n",
    "            self.a1 += 0.01\n",
    "            self.a3 -= 0.01\n",
    "            self.H -= 1\n",
    "\n",
    "        elif choice == 23:\n",
    "            self.move(x=1, y=-1)\n",
    "            self.a1 += 0.01\n",
    "            self.a3 -= 0.01\n",
    "            self.H -= 1\n",
    "            \n",
    "        elif choice == 24:\n",
    "            self.move(x=1, y=1)\n",
    "            self.a1 -= 0.01\n",
    "            self.a3 += 0.01\n",
    "            self.H -= 1\n",
    "            \n",
    "        elif choice == 25:\n",
    "            self.move(x=-1, y=-1)\n",
    "            self.a1 -= 0.01\n",
    "            self.a3 += 0.01\n",
    "            self.H -= 1\n",
    "\n",
    "        elif choice == 26:\n",
    "            self.a1 -= 0.01\n",
    "            self.a3 += 0.01\n",
    "            self.H -= 1\n",
    "\n",
    "        elif choice == 27:\n",
    "            self.move(x=1, y=-1)\n",
    "            self.a1 -= 0.01\n",
    "            self.a3 += 0.01\n",
    "            self.H -= 1\n",
    "            \n",
    "        elif choice == 28:\n",
    "            self.move(x=1, y=1)\n",
    "            self.a1 -= 0.01\n",
    "            self.a3 -= 0.01\n",
    "            self.H -= 1\n",
    "\n",
    "        elif choice == 29:\n",
    "            self.move(x=-1, y=-1)\n",
    "            self.a1 -= 0.01\n",
    "            self.a3 -= 0.01\n",
    "            self.H -= 1\n",
    "\n",
    "        elif choice == 30:\n",
    "            self.move(x=-1, y=1)\n",
    "            self.a1 -= 0.01\n",
    "            self.a3 -= 0.01\n",
    "            self.H -= 1\n",
    "\n",
    "        elif choice == 31:\n",
    "            self.move(x=1, y=-1)\n",
    "            self.a1 -= 0.01\n",
    "            self.a3 -= 0.01\n",
    "            self.H -= 1\n",
    "            \n",
    "        if self.a1 > 1:\n",
    "            self.a1 = 1\n",
    "        elif self.a1 < 0:\n",
    "            self.a1 = 0\n",
    "        if self.a3 > 1:\n",
    "            self.a3 = 1\n",
    "        elif self.a3 < 0:\n",
    "            self.a3 = 0\n",
    "        if self.H <= 10:\n",
    "            self.H =10\n",
    "        \n",
    "\n",
    "    def move(self, x=False, y=False):\n",
    "\n",
    "        if not x:\n",
    "            self.x += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.x += x\n",
    "\n",
    "        if not y:\n",
    "            self.y += np.random.randint(-1, 2)\n",
    "        else:\n",
    "            self.y += y\n",
    "\n",
    "        if self.x < 0:\n",
    "            self.x = 0\n",
    "        elif self.x > self.size-1:\n",
    "            self.x = self.size-1\n",
    "        if self.y < 0:\n",
    "            self.y = 0\n",
    "        elif self.y > self.size-1:\n",
    "            self.y = self.size-1\n",
    "\n",
    "class BlobEnv():\n",
    "    SIZE = 100\n",
    "    MOVE_PENALTY = 1\n",
    "    OBSERVATION_SPACE_VALUES = (SIZE, SIZE, 3)  # 4\n",
    "    UAV_N = 1  # UAV key in dict\n",
    "    USER_N = 2  # USER key in dict\n",
    "    UAV2_N = 4  # UAV2 key in dict\n",
    "    # the dict! (colors)\n",
    "    d = {1: (255, 175, 0),\n",
    "         2: (0, 255, 0),\n",
    "         3: (0, 0, 255),\n",
    "         4: (175, 0, 255)}\n",
    "\n",
    "    def reset(self):\n",
    "        P = 10**((p-30)/10) # Transmitted power 20dbm (i.e. .1w)\n",
    "        N_uav = 8\n",
    "        N_ue = 8\n",
    "        G = N_uav * N_ue\n",
    "        P *= G\n",
    "        W = 2e9 # Bandwidth 2GHz\n",
    "        fc = 28e9 # Carrier frequency = 28GHz\n",
    "        NF = 10**(5/10) # 5dB Noise Figure \n",
    "        TN = 10**(-114/10) # -84dBm Thermal Noise\n",
    "        N = NF * TN\n",
    "        C_LOS = 10**(-6.4)\n",
    "        a_LOS = 2\n",
    "        C_NLOS = 10**(-7.2) \n",
    "        a_NLOS = 2.92\n",
    "\n",
    "        self.UAV = Blob(self.SIZE)\n",
    "        self.UAV2 = Blob(self.SIZE)\n",
    "        self.h1 = []\n",
    "        self.h2 = []\n",
    "        self.h3 = []\n",
    "        self.h4 = []\n",
    "        self.a1 = []\n",
    "        self.a2 = []\n",
    "        self.a3 = []\n",
    "        self.a4 = []\n",
    "        self.SUM1 = []\n",
    "        self.SUM2 = []\n",
    "        self.SUM3 = []\n",
    "        self.SUM4 = []\n",
    "        self.Fairness = []\n",
    "        self.Hl = []\n",
    "        self.NLOS = []\n",
    "        self.NOMA = []\n",
    "        self.reward1 = []\n",
    "        self.reward2 = []\n",
    "        self.reward3 = []\n",
    "        self.reward4 = []\n",
    "        self.reward5 = []\n",
    "        self.reward6 = []\n",
    "        \n",
    "        self.UAV.a1 = 0.5\n",
    "        self.UAV.a2 = 0.5\n",
    "        self.UAV.a3 = 0.5\n",
    "        self.UAV.a4 = 0.5\n",
    "        self.UAV.H = 50\n",
    "        H2 = 50\n",
    "        \n",
    "        self.USER1 = Blob(self.SIZE, True, False, False, False)\n",
    "        self.USER2 = Blob(self.SIZE, False, True, False, False)\n",
    "        self.USER3 = Blob(self.SIZE, False, False, True, False)\n",
    "        self.USER4 = Blob(self.SIZE, False, False, False, True)\n",
    "\n",
    "        self.UAV2.x = int((self.USER1.x +self.USER2.x + self.USER3.x + self.USER4.x)/4)\n",
    "        self.UAV2.y = int((self.USER1.y +self.USER2.y + self.USER3.y + self.USER4.y)/4)\n",
    "        \n",
    "        \n",
    "        ob1 = self.UAV-self.USER1\n",
    "        ob2 = self.UAV-self.USER2\n",
    "        ob3 = self.UAV-self.USER3\n",
    "        ob4 = self.UAV-self.USER4\n",
    "        \n",
    "        D1 =  np.sum(np.sqrt([(10*ob1[0])**2, (10*ob1[1])**2]))\n",
    "        D2 = np.sum(np.sqrt([(10*ob2[0])**2, (10*ob2[1])**2]))\n",
    "        D3 = np.sum(np.sqrt([(10*ob3[0])**2, (10*ob3[1])**2]))\n",
    "        D4 = np.sum(np.sqrt([(10*ob4[0])**2, (10*ob4[1])**2]))\n",
    "                  \n",
    "        H = self.UAV.H\n",
    "        Dt1 = np.sum(np.sqrt([ (10*ob1[0])**2, (10*ob1[1])**2, H**2  ]))\n",
    "        Dt2 = np.sum(np.sqrt([ (10*ob2[0])**2, (10*ob2[1])**2, H**2  ]))\n",
    "        Dt3 = np.sum(np.sqrt([ (10*ob3[0])**2, (10*ob3[1])**2, H**2  ]))\n",
    "        Dt4 = np.sum(np.sqrt([ (10*ob4[0])**2, (10*ob4[1])**2, H**2  ]))\n",
    "        \n",
    "        self.L1 = mmLineOfSight_Check(Dt1,H)\n",
    "        self.L2 = mmLineOfSight_Check(Dt2,H)\n",
    "        self.L3 = mmLineOfSight_Check(Dt3,H)\n",
    "        self.L4 = mmLineOfSight_Check(Dt4,H)\n",
    "        \n",
    "        if self.L1 == 1:\n",
    "            h1 = C_LOS * Dt1**(-a_LOS)\n",
    "        else:\n",
    "            h1 = C_NLOS * Dt1**(-a_NLOS)\n",
    "\n",
    "        if self.L2 == 1:\n",
    "            h2 = C_LOS * Dt2**(-a_LOS)\n",
    "        else:\n",
    "            h2 = C_NLOS * Dt2**(-a_NLOS)\n",
    "        if self.L3 == 1:\n",
    "            h3 = C_LOS * Dt3**(-a_LOS)\n",
    "        else:\n",
    "            h3 = C_NLOS * Dt3**(-a_NLOS)\n",
    "        if self.L4 == 1:\n",
    "            h4 = C_LOS * Dt4**(-a_LOS)\n",
    "        else:\n",
    "            h4 = C_NLOS * Dt4**(-a_NLOS)\n",
    "        \n",
    "        a1 =  self.UAV.a1\n",
    "        a2 =  1 - a1\n",
    "        a3 =  self.UAV.a3\n",
    "        a4 =  1 - a3\n",
    "        observation = [ob1[0]] + [ob1[1]] + [ob2[0]] + [ob2[1]]+ [ob3[0]] + [ob3[1]]+ [ob4[0]] + [ob4[1]] + [a1] + [a2] + [a3] + [a4] + [h1] + [h2] + [h3] + [h4] + [H]\n",
    "        \n",
    "        self.episode_step = 0\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        done= False\n",
    "        \n",
    "        P = 10**((p-30)/10) # Transmitted power 20dbm (i.e. .1w)\n",
    "        N_uav = 8\n",
    "        N_ue = 8\n",
    "        G = N_uav * N_ue\n",
    "        P *= G\n",
    "        W = 2e9 # Bandwidth 2GHz\n",
    "        fc = 28e9 # Carrier frequency = 28GHz\n",
    "        NF = 10**(5/10) # 5dB Noise Figure \n",
    "        TN = 10**(-114/10) # -84dBm Thermal Noise\n",
    "        N = NF * TN\n",
    "        C_LOS = 10**(-6.4)\n",
    "        a_LOS = 2\n",
    "        C_NLOS = 10**(-7.2) \n",
    "        a_NLOS = 2.92        \n",
    "        H = self.UAV.H # antenna Height\n",
    "        \n",
    "        self.episode_step += 1\n",
    "        \n",
    "        ob1 = self.UAV-self.USER1\n",
    "        ob2 = self.UAV-self.USER2\n",
    "        ob3 = self.UAV-self.USER3\n",
    "        ob4 = self.UAV-self.USER4\n",
    "        \n",
    "        D1 =  np.sum(np.sqrt([(10*ob1[0])**2, (10*ob1[1])**2]))\n",
    "        D2 = np.sum(np.sqrt([(10*ob2[0])**2, (10*ob2[1])**2]))\n",
    "        D3 = np.sum(np.sqrt([(10*ob3[0])**2, (10*ob3[1])**2]))\n",
    "        D4 = np.sum(np.sqrt([(10*ob4[0])**2, (10*ob4[1])**2]))\n",
    "                  \n",
    "        H = self.UAV.H\n",
    "        Dt1 = np.sum(np.sqrt([ (10*ob1[0])**2, (10*ob1[1])**2, H**2  ]))\n",
    "        Dt2 = np.sum(np.sqrt([ (10*ob2[0])**2, (10*ob2[1])**2, H**2  ]))\n",
    "        Dt3 = np.sum(np.sqrt([ (10*ob3[0])**2, (10*ob3[1])**2, H**2  ]))\n",
    "        Dt4 = np.sum(np.sqrt([ (10*ob4[0])**2, (10*ob4[1])**2, H**2  ]))\n",
    "        \n",
    "        self.L1 = mmLineOfSight_Check(Dt1,H)\n",
    "        self.L2 = mmLineOfSight_Check(Dt2,H)\n",
    "        self.L3 = mmLineOfSight_Check(Dt3,H)\n",
    "        self.L4 = mmLineOfSight_Check(Dt4,H)\n",
    "        \n",
    "        if self.L1 == 1:\n",
    "            h1 = C_LOS * Dt1**(-a_LOS)\n",
    "            self.NLOS.append(0)\n",
    "        else:\n",
    "            h1 = C_NLOS * Dt1**(-a_NLOS)\n",
    "            self.NLOS.append(1)\n",
    "        if self.L2 == 1:\n",
    "            h2 = C_LOS * Dt2**(-a_LOS)\n",
    "            self.NLOS.append(0)\n",
    "        else:\n",
    "            h2 = C_NLOS * Dt2**(-a_NLOS)\n",
    "            self.NLOS.append(1)\n",
    "        if self.L3 == 1:\n",
    "            h3 = C_LOS * Dt3**(-a_LOS)\n",
    "            self.NLOS.append(0)\n",
    "        else:\n",
    "            h3 = C_NLOS * Dt3**(-a_NLOS)\n",
    "            self.NLOS.append(1)\n",
    "        if self.L4 == 1:\n",
    "            h4 = C_LOS * Dt4**(-a_LOS)\n",
    "            self.NLOS.append(0)\n",
    "        else:\n",
    "            h4 = C_NLOS * Dt4**(-a_NLOS)\n",
    "            self.NLOS.append(1)\n",
    "        \n",
    "        self.UAV.action(action)\n",
    "        \n",
    "        a1 =  self.UAV.a1\n",
    "        a2 =  1 - a1\n",
    "        a3 =  self.UAV.a3\n",
    "        a4 =  1 - a3\n",
    "        \n",
    "        self.h1.append(h1)\n",
    "        self.h2.append(h2)\n",
    "        self.h3.append(h3)\n",
    "        self.h4.append(h4)\n",
    "        self.a1.append(a1)\n",
    "        self.a2.append(a2)\n",
    "        self.a3.append(a3)\n",
    "        self.a4.append(a4)\n",
    "        self.Hl.append(H)\n",
    "\n",
    "        reward = 0\n",
    "        reward_1 = 0\n",
    "        reward_2 = 0\n",
    "        reward_4 = 0\n",
    "        reward_5 = 0\n",
    "        reward_6 = 0\n",
    "        \n",
    "        if h1 >= h2:\n",
    "            \n",
    "            SUM1 = math.log2(1 + h1 * a1 * P/N)\n",
    "            SUM2 = math.log2(1 + a2 * h2 * P / (a1 * h2 * P + N) )\n",
    "            reward_1 += SUM1\n",
    "            reward_2 += SUM2\n",
    "\n",
    "        else: \n",
    "        \n",
    "            SUM1 = math.log2(1 + a1 * h1 * P / (a2 * h1 * P + N) )\n",
    "            SUM2 =  math.log2(1 + h2 * a2 * P/N)\n",
    "            reward_1 += SUM2\n",
    "            reward_2 += SUM1\n",
    "                \n",
    "        if h3 >= h4:\n",
    "            SUM3 = math.log2(1 + h3 * a3 * P/N)\n",
    "            SUM4 = math.log2(1 + a4 * h4 * P / (a3 * h4 * P + N) ) \n",
    "            reward_4 += SUM3\n",
    "            reward_5 += SUM4\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            SUM3 = math.log2(1 + a3 * h3 * P / (a4 * h3 * P + N) )\n",
    "            SUM4 = math.log2(1 + h4 * a4 * P/N)\n",
    "            reward_4 += SUM4\n",
    "            reward_5 += SUM3  \n",
    "        \n",
    "        reward_3 = (SUM1 + SUM2 + SUM3 + SUM4)**2 / (4 * (SUM1**2 + SUM2**2 + SUM3**2 + SUM4**2))\n",
    "        self.Fairness.append(reward_3)\n",
    "\n",
    "        self.SUM1.append(SUM1)\n",
    "        self.SUM2.append(SUM2)\n",
    "        self.SUM3.append(SUM3)\n",
    "        self.SUM4.append(SUM4)\n",
    "\n",
    "        r = 0\n",
    "\n",
    "        if SUM1 >= r:\n",
    "            reward += 100\n",
    "        if SUM2 >= r:\n",
    "            reward += 100\n",
    "        if SUM3 >= r:\n",
    "            reward += 100\n",
    "        if SUM4 >= r:\n",
    "            reward += 100\n",
    "\n",
    "        if reward >= 400:\n",
    "          SUM1*=10\n",
    "          SUM2*=10\n",
    "          SUM3*=10\n",
    "          SUM4*=10\n",
    "\n",
    "        reward_3 *= 0\n",
    "        reward_6 += 2e10 * (h1+h2+h3+h4) * 0 \n",
    "        reward +=   (SUM1 + SUM2 + SUM3 + SUM4)  + reward_3  + reward_6\n",
    "        \n",
    "\n",
    "        self.reward1.append(reward_1)\n",
    "        self.reward2.append(reward_2)\n",
    "        self.reward3.append(reward_3)\n",
    "        self.reward4.append(reward_4)\n",
    "        self.reward5.append(reward_5)\n",
    "        self.reward6.append(reward_6)\n",
    "\n",
    "        new_observation_m =  ([ob1[0]] + [ob1[1]] + [ob2[0]] + [ob2[1]]+ [ob3[0]] + [ob3[1]] + [ob4[0]] + [ob4[1]] + [a1] + [a2] + [a3] + [a4] + [h1] + [h2] + [h3] + [h4] + [H] )\n",
    "        new_observation =  new_observation_m  \n",
    "        if self.episode_step >= 300:\n",
    "\n",
    "            ob21 = self.UAV2-self.USER1\n",
    "            ob22 = self.UAV2-self.USER2\n",
    "            ob23 = self.UAV2-self.USER3\n",
    "            ob24 = self.UAV2-self.USER4\n",
    "            H2 = 50\n",
    "            \n",
    "            D21 =  np.sum(np.sqrt([(10*ob21[0])**2, (10*ob21[1])**2]))\n",
    "            D22 = np.sum(np.sqrt([(10*ob22[0])**2, (10*ob22[1])**2]))\n",
    "            D23 = np.sum(np.sqrt([(10*ob23[0])**2, (10*ob23[1])**2]))\n",
    "            D24 = np.sum(np.sqrt([(10*ob24[0])**2, (10*ob24[1])**2]))\n",
    "\n",
    "            Dt21 = np.sum(np.sqrt([ (10*ob21[0])**2, (10*ob21[1])**2, H2**2  ]))\n",
    "            Dt22 = np.sum(np.sqrt([ (10*ob22[0])**2, (10*ob22[1])**2, H2**2  ]))\n",
    "            Dt23 = np.sum(np.sqrt([ (10*ob23[0])**2, (10*ob23[1])**2, H2**2  ]))\n",
    "            Dt24 = np.sum(np.sqrt([ (10*ob24[0])**2, (10*ob24[1])**2, H2**2  ]))\n",
    "        \n",
    "            h221 = C_LOS * Dt21**(-a_LOS)\n",
    "            h222 = C_LOS * Dt22**(-a_LOS)\n",
    "            h223 = C_LOS * Dt23**(-a_LOS)\n",
    "            h224 = C_LOS * Dt24**(-a_LOS)\n",
    "\n",
    "            if h221 >= h222:\n",
    "                a222 = ((2**r - 1)/2**r) * (1 + N/(P*h222))\n",
    "                if a222 >= 1:\n",
    "                  a222 = 1\n",
    "                a221 = 1 - a222\n",
    "                SUM221 = math.log2(1 + h221 * a221 * P/N)\n",
    "                SUM222 = math.log2(1 + a222 * h222 * P / (a221 * h222 * P + N) )\n",
    "            else: \n",
    "                a221 = ((2**r - 1)/2**r) * (1 + N/(P*h221))\n",
    "                if a221 >= 1:\n",
    "                  a221 = 1\n",
    "                a222 = 1-a221\n",
    "                SUM221 = math.log2(1 + a221 * h221 * P / (a222 * h221 * P + N) )\n",
    "                SUM222 =  math.log2(1 + h222 * a222 * P/N)\n",
    "            if h223 >= h224:\n",
    "\n",
    "                a224 = ((2**r - 1)/2**r) * (1 + N/(P*h224))\n",
    "                if a224 >= 1:\n",
    "                  a224 = 1\n",
    "                a223 = 1 - a224\n",
    "                SUM223 = math.log2(1 + h223 * a223 * P/N)\n",
    "                SUM224 = math.log2(1 + a224 * h224 * P / (a223 * h224 * P + N) ) \n",
    "            else: \n",
    "                a223 = ((2**r - 1)/2**r) * (1 + N/(P*h223))\n",
    "                if a223 >= 1:\n",
    "                  a223 = 1\n",
    "                a224 = 1 - a223\n",
    "                SUM223 = math.log2(1 + a223 * h223 * P / (a224 * h223 * P + N) )\n",
    "                SUM224 = math.log2(1 + h224 * a224 * P/N)\n",
    "                \n",
    "            average_sum_rate2 =  SUM221 + SUM222 + SUM223 + SUM224  \n",
    "            Fairness222 = (SUM221 + SUM222 + SUM223 + SUM224)**2 / (4 * (SUM221**2 + SUM222**2 + SUM223**2 + SUM224**2))\n",
    "\n",
    "            \n",
    "            h11.append(Average(self.h1))\n",
    "            h22.append(Average(self.h2)) \n",
    "            h33.append(Average(self.h3)) \n",
    "            h44.append(Average(self.h4)) \n",
    "            a11.append(Average(self.a1)) \n",
    "            a22.append(Average(self.a2)) \n",
    "            a33.append(Average(self.a3)) \n",
    "            a44.append(Average(self.a4)) \n",
    "            SUM11.append(Average(self.SUM1)) \n",
    "            SUM22.append(Average(self.SUM2)) \n",
    "            SUM33.append(Average(self.SUM3)) \n",
    "            SUM44.append(Average(self.SUM4))\n",
    "            reward1.append(Average(self.reward1))\n",
    "            reward2.append(Average(self.reward2))\n",
    "            reward3.append(Average(self.reward3))\n",
    "            reward4.append(Average(self.reward4))\n",
    "            reward5.append(Average(self.reward5))\n",
    "            reward6.append(Average(self.reward6))\n",
    "            average_episode_reward = episode_reward/self.episode_step \n",
    "            Fairnessl.append(Average(self.Fairness))\n",
    "            episode_rewards.append(average_episode_reward)\n",
    "            episode_durations.append(timestep)\n",
    "            Height.append(Average(self.Hl))\n",
    "            AVG2.append(average_sum_rate2)\n",
    "            Fairnessl_2.append(Fairness222)\n",
    "\n",
    "            done = True\n",
    "            \n",
    "            plot(episode_rewards,reward1,reward2,reward3,reward4,reward5,reward6,h11,h22,h33,h44,a11,a22,a33,a44,SUM11,SUM22,SUM33,SUM44,Fairnessl,Height,AVG2,Fairnessl_2, 100)\n",
    "            \n",
    "            if episode >=999:\n",
    "              average_h1 = 10 * math.log10(h11[-1])\n",
    "              average_h2 = 10 * math.log10(h22[-1])\n",
    "              average_h3 = 10 * math.log10(h33[-1])\n",
    "              average_h4 = 10 * math.log10(h44[-1])\n",
    "\n",
    "              average_h21 = 10* math.log10(h221)\n",
    "              average_h22 = 10* math.log10(h222)\n",
    "              average_h23 = 10* math.log10(h223)\n",
    "              average_h24 = 10* math.log10(h224)\n",
    "            \n",
    "              average_sum_rate = SUM11[-1] + SUM22[-1] + SUM33[-1] + SUM44[-1]\n",
    "            \n",
    "\n",
    "              print(\"\\n                          UAV2                            \")\n",
    "              print(\"Sum Rate:\", round(2*average_sum_rate2, 2), \"Gbps, Total SE = \", round(average_sum_rate2, 2), \" EE = \", round(average_sum_rate2/(P),2))\n",
    "              print(\"SE1: \",round(SUM221, 2),\"Bits/s/Hz, SE2: \",round(SUM222, 2),\"Bits/s/Hz, SE3: \",round(SUM223, 2),\"Bits/s/Hz, SE4: \",round(SUM224, 2),\"Bits/s/Hz\")\n",
    "            \n",
    "\n",
    "                          \n",
    "        return new_observation,new_observation_m, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        img = self.get_image()\n",
    "        img = img.resize((500, 500)) # resizing\n",
    "        cv2.imshow(\"UAV Beta 0.95\", np.array(img)) \n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    def get_image(self):\n",
    "        env = np.full((self.SIZE, self.SIZE, 3), 255, dtype=np.uint8)  # starts an rbg img\n",
    "        env[self.USER1.x][self.USER1.y] = self.d[(self.L1+1)]  \n",
    "        env[self.USER2.x][self.USER2.y] = self.d[(self.L2+1)]\n",
    "        env[self.USER3.x][self.USER3.y] = self.d[(self.L3+1)] \n",
    "        env[self.USER4.x][self.USER4.y] = self.d[(self.L4+1)]\n",
    "        env[self.UAV.x][self.UAV.y] = self.d[self.UAV_N]\n",
    "        img = Image.fromarray(env, 'RGB')\n",
    "        return img \n",
    "\n",
    "batch_size = 128\n",
    "gamma = 0.999\n",
    "eps_start = 0.9\n",
    "eps_end = 0.05\n",
    "eps_decay = 200\n",
    "target_update = 10\n",
    "memory_size = 15000\n",
    "lr = 0.001\n",
    "num_episodes = 1000\n",
    "num_of_actions = 32\n",
    "num_of_arg_per_state = 17\n",
    "SHOW_PREVIEW = False\n",
    "AGGREGATE_STATS_EVERY = 10\n",
    "\n",
    "\n",
    "for p in [10]:\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    em = BlobEnv()\n",
    "    strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "    agent = Agent(strategy, num_of_actions, device)\n",
    "    memory = ReplayMemory(memory_size)\n",
    "    policy_net = DQN(num_of_arg_per_state).to(device)\n",
    "    target_net = DQN(num_of_arg_per_state).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "\n",
    "    episode_durations = []\n",
    "    episode_rewards = []\n",
    "    episode_wins = []\n",
    "    h11 = []\n",
    "    h22 = []\n",
    "    h33 = []\n",
    "    h44 = []\n",
    "    a11 = []\n",
    "    a22 = []\n",
    "    a33 = []\n",
    "    a44 = []\n",
    "    SUM11 = []\n",
    "    SUM22 = []\n",
    "    SUM33 = []\n",
    "    SUM44 = []\n",
    "    TOTAL_SUM = []\n",
    "    Fairnessl = []\n",
    "    Height = []\n",
    "    reward1 = []\n",
    "    reward2 = []\n",
    "    reward3 = []\n",
    "    reward4 = []\n",
    "    reward5 = []\n",
    "    reward6 = []\n",
    "    AVG2 = []\n",
    "    Fairnessl_2 = []\n",
    "\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = torch.tensor([em.reset()], dtype=torch.float32).to(device)\n",
    "        episode_reward = 0\n",
    "        episode_win = 0\n",
    "\n",
    "        for timestep in count():   \n",
    "            action = agent.select_action(state, policy_net)\n",
    "            next_state, next_state_m, reward, done = em.step(action.item())\n",
    "            episode_reward += reward\n",
    "            reward = torch.tensor([reward], dtype=torch.int64).to(device)\n",
    "            next_state = torch.tensor([next_state], dtype=torch.float32).to(device)\n",
    "            next_state_m = torch.tensor([next_state_m], dtype=torch.float32).to(device)        \n",
    "            memory.push(Experience(state, action, next_state_m, reward))\n",
    "            state = next_state\n",
    "\n",
    "            if memory.can_provide_sample(batch_size):\n",
    "                experiences = memory.sample(batch_size)\n",
    "                states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "                current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "                next_q_values = QValues.get_next(target_net, next_states)\n",
    "                target_q_values = (next_q_values * gamma) + rewards\n",
    "                loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n",
    "                em.render()\n",
    "                \n",
    "            if done:         \n",
    "                break\n",
    "\n",
    "        if episode % target_update == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            \n",
    "torch.save(policy_net, f'AdaptSky-EE-Model-r=0-p={p}dBm.pt')\n",
    "print(\"\\nModel Saved Successfully\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
